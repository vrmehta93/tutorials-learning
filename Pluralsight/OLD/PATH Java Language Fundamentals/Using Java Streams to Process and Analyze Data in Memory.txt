Jose Paumard

2.1 - Introducing the Course and Its Agenda (Processing Data Using the Map Filter Reduce Algorithm)
	Java Stream API introduced in Java 8 and updated in the subsequent version of Java. And this API, is in fact, an implementation provided by the JDK of the map/filter/reduce algorithm
	So before we describe the API, we're going to describe this algorithm quite precisely and see how it can be implemented and the mistakes you want to avoid when implementing this algorithm
	Stream API is an in-memory implementation of the map/filter/reduce algorithm available in the JDK

2.2 - Who Are you? What Should You Know to Follow This Course?

2.3 - Module Agenda, Using the Map/Filter/Reduce Algorithm

2.4 - Introducing the Mapping, the Filtering, and the Reduction steps
	Let us start by defining what is this map/filter/reduce algorithm, and for that we're going to take a very simple example
	Suppose we have a list of people, instances of some kind of person, and we want to compute the average value of the age of those people, but not all the people we have in our list, only the people that are older than, let's say 20 years old
	This is a very classical use case
	If you take a closer look, there is a first step, that is take a person and compute the age of that person. Now, in most cases, this age would be a property of the person but basically transform a person object to an age. This age could be anything, really, it's just the fact that you take an entity and you transform this entity in some other thing. And this is called the mapping step. Mapping takes an object and returns another object of another type
	The second step takes this age, and it will check if the age is okay, decide whether this age should be kept or not. This is the filter step. The filter step takes an object, it doesn't transform it, it's very different from the mapping step, but it decides if the subject should be kept for further computation
	Now this reduce step is in fact very subtle and we're going to talk about it much more detail later in this course. So for the moment, we're just going to say that this reduce step is basically an aggregation

2.5 - Defining the Mapping and the Filtering Steps
	Mapping takes a list of given entities, here there are instances of List<Person> -> List<Integer>
	A mapping changes the type of the data, does not change the number of elements.
	A filter doesn't change the type of the data, it does change the number of elements

2.6 - Implementing Map/Filter/Reduce with the Iterator Pattern
	(Example of basic code implementing map/filter/reduce) Now if we take a look at this code, what does it do? What the code does exactly? This code, in fact, describe how this computation is conducted, how this map/filter/reduce stuff is implemented. So I'm not writing a code that describes a result. I'm writing a code that implements an algorithm, and if I need to change this algorithm, well, I need to change my code, even if the result of the code is the same
	It doesn't necessarily have to be like that, right? If you check the SQL language, for instance, you can see that in SQL what you're writing is a description of the results and not a description on how to compute that result
	So we could say from that perspective, the SQL langauge does a much better job than our code because our code describes how to compute things, and the SQL language only describe what should be computed and leaves the implementation, leaves the algorithm to the database server itself

2.7 - Trying to Implement Map/Filter/Reduce on the Collection API
	The first idea you may have would be, our data is in a list or in a collection, what about implementing a map or filter method directly on the collection framework
	And that could work, by the way, because if we just add this method map, we can see that this mapping is in fact modeled by a function from java.util.function, so the function that takes an object and returns an object of another type. And this filter maybe modeled by your Predicate also from the java.util.function package
	But the fact is, it hsa not been implemented like that. If you try to type that code in your IDE, it will just not compile

2.8 - Analyze the Map/Filter/Reduce Implementation on the Collection API
	Let us suppose one moment that we have this map and filter method directly on the collection interface. If we want to map this list of people, we're going to map it by calling this map method and pass this function as a parameter
	But if we have a big list of people, let us suppose 1MM people, then it will duplicate this 1MM people in a list 1MM integers, 1MM ages, becase we know that the mapping keeps the number of elements. If I have 1MM elements at the beginning, I will have 1MM elements at the end
	So it makes this mapping step extremely costly, memory wise, and also CPU wise because creating a list of 1MM integers is not free for the CPU. It will create a huge load on the CPU and also on the memory. So it's probably not a very good approach to do that 
	If we take a look at the filter step, it turns out that the filter step is not that bad. So if we have 1MM ages, maybe at the end of this step, we will end up with 600k ages something like thta. But it is still a huge duplication with a heavy load on the memory and on the CPU
	So this way of implementing the map and the filter step is, in fact, very costly and also much more costly than the first implementation we saw, which was based on the use ofthis iterative pattern
	If we want to create an efficient implementation of the map/filter/reduce, it should NOT duplicate any data, just to work in the same way, performance wise, as this iterative pattern

2.9 Designing the Stram API to Avoid Duplicating the Processed Data
	And the conclusion that was taken away from that is that the collection framework is not the right place to implement the map/filter/reduce algorithm and this is why this Stream object has been introduced.
	So the stream object is an object that implements map/filter/reduce without any duplication. This is how this object has been constructed
	Let's take a closer look at what's happening undre the hood. We have our List<People> and I have as many people as I need in that list. I call the stream() method on that list, and by doing so I create a Stream object
	But as we saw, a Stream object doesn't duplicate any data, so within that Stream object, inside that Stream object, I have nothing. A stream by definition is an empty object. It doesn't carry any kind of data
	So creating that Stream object is basically free. Calling list.streams() returns a Stream object, and building that Stream object is instantaneous. It doesn't create any load, neither on the memory, neither on the CPU
	This is the key point of the Stream API

2.10 - Implementing Map/Filter/Reduce on the Stream API
	Let's go back to our example, we have our list of people, we want to create a stream on this list of people. So for that, we just call the stream()  on this list and this is going to return a Stream object
	Now if we want to map this stream, we just call the map() method, pass a function that models the mapping as a parameter and this is going to create a new Stream Object.
	Every time you call a method on a stream that returns another stream, it is going to be a new stream object, but remember, there is no data in a stream. So since there is no data, creating a second stream is also free, it doesn't create any load, neither on CPU, neither on memory
	And we can do the same for filter(). filter() is another method on the Stream API that takes a Predicate as a parameter and that is going to model this filter step. And this filter () also returns a stream, which is a new stream
	At the end of the day, you're going to call a reduction method. In this case, I'm going to call the average() method. This average() does exist but it exists only on special streams that we're going to see later.
	If you type this code in your IDE, it won't work as is
	But this average method does not return a stream, it is supposed to return the result, which is the average value of the ages I want to process. So this time, it's not a new stream, it's really a result. And this method that returns a result triggers the computation of all the people, and those people are going to be taken one-by-one, first mapped, then filtered
	So all the data from the list of person is going to be processed one-by-one, one at a time exactly in the same way as in the iterative pattern. If you compare this pattern with the iterative pattern, you will see that they are roughly working in the same way. Difference is that when I write this code, what I'm really doing is I'm describing what this computation should do, a map, a filter and the computation of average and not how this computation should be conducted. This is none of my business, this is the business of the API

2.11 - Defining Intermediate and Terminal Operations on Streams
	From that, you can see that there are 2 kinds of methods defined on the stream object. The first kind of method is a method that returns another stream
	And then there are other kinds of methods which produce a result. Those methods are methods that return something else than a Stream
	The definition is that a method that creates a Stream is an intermediate method. And the method that produces a result is called the terminal method/operation
	So if you have a pattern using a Stream that doesn't end up with a terminal method, your pattern is not going to process any data, it's not going to do anything. So this is useless code. Most probably buggy code. So you can safely remove it from your app

3.1 - Introducing the Module And Its Agenda (Using the Stream API to Map, Filter, and Reduce Data)

3.2 - Live Demo: Writing a First Stream that Counts Empty Names

3.3 - Live Demo: Getting Errors When Processing the Same Stream Twice
	This is one of the key point and quite subtle point of the Stream API, you are not allowed to process the same stream twice, whether it is to call an intermediate operation on it or a terminal operation on it
	You'll runtime error if you do
	And this is why, in fact, in real applications in the real world, we do not streams in that way (defining intermediate variables). It is completely useless  to create intermittent variables to store the streams, so we prefer most of the time to inline all this

3.4 - Streaming Related Entities with the FlatMap Operator
	Flat mapping a stream is about dealing with one-to-many relationships between entities
	So let us first see that on an example: Suppose we have a list of cities - New York, Paris, London and for each city, we have a list of people that are living in those cities. In this example, it is a list but it could be really any kind of relationship. A map would work or another way of implementing this relationship would also work
	And what I would like to extract from this list is in fact the list of all the people whether they live in NY, Paris, London.
	So I have a list of entities, my cities. Each city has their one-to-many relationship with a person instance. And from this list of city, I'm interested in the list of related entities and I'm not interested in the list of cities anymore
	We're in the stream world. So we'll deal with streams. On the one hand, we have a Stream<City> and what we want is a Stream<Person>. Now what we have also is a relationship between one city instance and several instances of this Person class. So let us make that a function that takes a city and that returns not a list of people, but a stream of people
	Why? Because we're in the stream world, so we're dealing with streams. The flatMap() operation does exactly that. It takes that function that takes an entity, city, and returns a stream of other entities, person, and from that stream of city, it is going to be able to create a stream of the instances of person related to those cities
	It's kind of magic and it just works like that
	Code:
		List<City> cities = ...;
		
		Function<City, Stream<Person>> flatMapper = city -> city.getPeople().stream();
		
		long count = cities.stream()	// Stream<City>
						.flatMap(flatMapper)	// Stream<Person>
						.count();
	The key point you need to remember with this flat mapper is that a flatMap operation is defined by a special function that takes an object and returns a stream of other objects

3.5 - Live Demo: Flat Mapping Related Entities from a Stream of Entities
	forEach()

4.1 - Introducing the Module and Its Agenda (Building a Stream from Data in Memory)

4.2 - Live Demo: Creating Streams from an Array of Objects
	To create a stream from an array of Objects, we have 2 patterns for that:
		One relies on the factory method of the Arrays class - Arrays.stream(people);
		Another factory method, which is the method defined on the Stream interface = Stream.of(people);
	Example:
		Person p01 = new Person(..);
		Person p02 ...
		Person p05
		
		Person[] people = {p01, ... p05};
		
		long count = Stream.of(people).count();
		Arrays.stream(people).forEach(p -> sysout(p));
		// BTW "p -> sysout(p)" lambda expression can be returned as a method reference. You can ask your IDE to make this conversion
		Arrays.stream(people).forEach(System.out::println);

4.3 - Live Demo: Streaming the Lines of a Text File
	The second pattern I would like to show you is about taking a text file and creating a stream that will process all the lines of the text file one by one. This is a very useful pattern that you can use really in many places in your app:
		main(){
			Path path = Path.of([filepath]);
			// Since you're dealing with opening a file, I need to take care of 2 things
			// First thing is the handling of the exception - FileNotFoundException and IOException
			// And I also need to take care of the closing of this IO resource once I'm done with it
			
			try(Stream<String> lines = Files.lines(path)) {
				long count = lines.count();
				sysout(count);
			}
			catch ...
		}
	So you can see that this is a way of creating a stream on an input/output resource in a very simple and very handy way

4.4 - Live Demo: Streaming the Splitting of a String
	Next pattern - I have this sentence "the quick brown fox jumps over the lazy dog" and I would like to split the sentence into words
	I have a very classical pattern to do that in the JDK, which is just the split() which returns an array
		main(){
			String sentence ...;
			String[] words = sentence.split(" ");
			Stream<String> wordStream = Arrays.stream(words);
			
			long count = wordStream.count();
			sysout(count);
		}
	What I would like you to understand in this example is the following - here we are splitting this sentence with this regular expression, creating the array. So we're paying the price of the splitting of the old sentence and the price of the creation and the storing of the result in an array, and then we're counting the result. We do not need the array anymore
	And this is really a pity because precisely the stream API is there to avoid storing in memory of intermediate results. And here we're just storing this array as an intermediate result. That is, in fact, not used as it is in my pattern
	There is a better way to do that based on this Pattern object, which is an object from the JDK used to model regular expression:
		main(){
			// Cont. from above
			
			Pattern pattern = Pattern.compile(" ");	// Same regex used to split the sentence
			long count2 =  pattern.splitAsStream(sentence).count();	// From this pattern object, we can create a stream directly with this splitAsStream()
		}
	The difference between the 2 ways, is that in the second way, there is no construction of any array, there is no pressure on the memory of my application, and there is less pressure on the CPU since the creation of the array is not completely free CPU wise
	So this pattern is much better than the previous one and much more efficient, as it is the case most of the time when you're using the stream API

4.5 - Live Demo: Streaming the Letters of a String
	Pattern that splits any string of characters into its letters
		main() {
			String sentence ...
			
			sentence.chars().mapToObj(codePoint -> Character.toString(codePoint))
				.filter(letter -> !letter.equals(" "))	// remove space
				.distinct()	// remove duplicates
				.sorted()	// sort in alphabetical order
				.forEach(letter -> sysout(letter));
		}
	This chars() method returns a special type of stream which is an IntStream. In fact, Stream<T> is not the only itnerface that models streams in the Stream API. I have 3 more interfaces, which are specialized interfaces to deal with streams of numbers. And here the IntStream is a stream that deals with int as primitive type, compared to the wrapping type associated to the int primitive type, compared to the wrapping type associated to the int primitive type, which is the Interger class
	So here what we have is an IntStream with a map and a filter method but with also a specialized method to deal with numbers explicitly
	I would like to convert this IntStream to a Stream<String>, each string containing a given letter of that sentence. To do that, I have a special pattern that usees this mapToObj() method from the IntStream space to the Stream<T> space. And this mapToObj method take in fact the ASCII code of the letter in the form of an int primitive type, and we're not going to call it ASCII code, but codePoint. And to make it a string we have a factory method on this Character class, which is called toString() that takes the codePoint as a parameter
	You can see this chars() method is extremely useful when you have to process strings of characters and when you want to process them letters by letters. This is a very useful method that you will see used in many apps

4.6 - Live Demo: Selecting Ranges of the Elements of a Stream
	You can in fact, from within a stream, select cretain portion of the stream based on the index of the elements of that stream
	And we're also going to see hwo you can select portions of the stream based on a predicate. A predicate can be used to filter a stream, and this is what we saw, but it can also be used to start the consumption of the elements of the stream and to stop this consumption
	Let us first see how those skip and limit methods are working and you're going to see there's a little trap here :
		main() {
			IntStream.range(0, 30)	// factory method from the IntStream interface. That is creating a stream of integers starting at 0 and ending at 29 because last element is not included in the stream
			.forEach(index -> sysout(index);
		}
	What I would like to do for instance is print out all the elements from 10 to 20. For that
		IntStream.range(0,30)
			.skip(10)
			.limit(20)	// Mistake
			.forEach(index -> sysout(index))	// prints "10 11 ... 29"
	skip() is going to tell the element I'm going to skip and I have a limit method that is going to say I only need certain amount of elements from that stream. Now the most frequent mistake you can see here is saying I need the element from 10 to 20, so I'm going to say skip(10) and limit(20)
	Limit is called on the stream. That is the stream written by the skip method here. And the stream written by teh skip method is a stream that is going to consume the integers from 10 to 30. So if you say that you're goin to limit to 20 elements, what you're going to have is 20 elements, starting at 10.
	To fix this:
		IntStream.range(0,30)
			.skip(10)
			.limit(10)	// Fixed
			.forEach(index -> sysout(index))	// prints "10 11 ... 19"
	(Using the File reading example) Here, we saw the skip and limit method on the IntStream object, but we also have them on the regular Stream<T> object
		main() {
			Path path = Path.of([filepath]);
			
			try(Stream<String> lines = Files.lines(path);) {
				lines.skip(20)
					.limit(10)
					.forEach(System.out::println);
			} catch ...
		}
	Just keep in mind that they are called on a stream returned by the previous method call. So this is quite important, especially if you're playing with indexes

4.7 - Live Demo: Using a Predicate to Control the Closing of a Stream
	We saw how to select elements from a stream based on the index of those elements in a sream. There is another way of selecting elements by opening and closing the stream based on a predicate. In fact, we have 2 methods for that
	The first one is called dropWhite and the other one is called takeWhile. Both of those methods are taking a predicate as a parameter
	TakeWhile is going to consume the elements of the stream until the predicate becomes false, and dropWhile does the opposite. The stream will be consumed, and until the predicate becomes true, the element will not be taken into account. And once this predicate becomes true, then the elements are going to be consumed by the stream pipeline of operations
	main() {
		Class<?> clzz = ArrayList.class;
		
//		Stream<Class<?>> classes = 
			Stream.<Class<?>>iterate(clzz, c -> c.getSuperclass())
				.forEach(System.out::println)
	}
	You may be aware that on this class object, we have a getSuperclass() method that returns the only superclass of the given class
	We're going to create a stream using a factory method called iterate() that takes a seed, and the second element is a unary operator. A unary operator, in that case, takes a class and returns another class
	How does the iterate method work? It takes the clzz and that is the first element that is going to be consumed by that stream, and then it will take the elment of the stream and feed that element to this unary operator to generate the next element
	So the first element is going to be release, and then superclass will be released, and then the superclass of the superclass until we reach the object.class
	So the superclass of object is null, and at some point we're going to call getSuperclass() on a null class. So if I run this code, indeed I will have ArrayList and its superclass that are going to be displayed but I also have a NullPointerException because the stream doesn't stop
	How can I stop it? One of the WRONG way of stopping it could be using the filter():
		main{
			...
			Stream<Class<?>>iterate(clzz, c -> c.getSuperclass())
				.filter(c -> c != null)
				.forEach(sysout)
		}
	It turns out when you run this code, it doesn't fix your problem at all because in fact what this filter step does is that it takes a class, and if this class is null, it's not going to transmit this class to the next step of the stream, which is displaying of the null class. But this binary operator is still called with this null value because we did not stop the stream
	The right way of stopping it is to call the takeWhile method that takes a predicate as a parameter
		main() {
			...
			Stream<Class<?>>iterate(clzz, c-> c.getSuperclass())
				.takeWhile(c -> c!= null)
				.forEach(sysout)
		}
	Suppose that in the future non-null values are going to be generated. It doesn't matter because the stream has been closed and there is no way to open it again. So this is how this takeWhile method works

5.1 - Introducing the Module and Its Agenda (Converting a For Loop to a Stream)

5.2 - Refactoring a Simple For Loop to a Stream Implementation
	Looking at initial example of iterative pattern to convert into stream:
		main(){
			Person p01 ...
			Person p05
			
			List<Person> people = List ...
			
			people.stream()
				.map(p -> p.getAge())
				.filter(age -> age > 20)
		}
	Now, what I want to do at the end of the day is compute an average and what I need to know is that I have an average method somewhere in the Stream API along with max, min, and sum but it's not defined on the Stream<T> Interface. Stream<T> deals with any kind of object and it may not be relevant to compute the average of people for instance
	So, this average method is not defined on this interface. This average method has been defined on the specialized streams, the streams of numbers. So on IntStream, LongStream, and DoubleStream I have this average method, which is not available here
	So what I need to do is convert this Stream<Integer> to an IntStream. Now the difference between both may look a little subtle but you know in Java, an Integer is not an int. Integer is a class, int is a primitive type. And to do that, what I need to do is map this stream to an IntStream. It's a mapping operation, and this is done at the mapping level
	Instead of calling map(), which takes a function that takes a T and returns any type, R, which happens to be an Integer in our case, I can call mapToInt() and mapToInt() is different. mapToInt() takes a ToIntFunction<T>. That is a function that takes any object and that returns an int as a primitive type
	So now the API knows that I'm dealing with numbers, in that case, int:
		main(){
			...
			OptionalDouble average2 = people.stream()
				.mapToInt(p -> p.getAge())	// Returns IntStream
				.filter(age -> age > 20)
				.average();	// Now you're able to call average 
		}
	If I put the results in a variable, another surprise awaits me. It returns an OptionalDouble
	What is an OptionalDouble. It's a specialized type of Optional. Optional is a wrapper object that may or may not wrap another object. I have Optional<T> and 3 specialized optional, OptionalDouble, OptionalInt, OptionalLong
	What the API tells me is that this average may not be defined, so it is wrapped in an object that may be empty. And after that, it's up to me and up to my application to decide what to do in case this optional is empty
	In our case, we know that in our stream of person, we have peple older than 20, so this Optional will not be empty. So what I can do right now is just call orElseThrow() which is going to return the value wrapped by that Optional, if there is a value, and throws a NoSuchElementException in the other case
	And now I can say that, yes, this is my average value
		main(){
			...
			double average2 = people.stream()
				.mapToInt(Person::getAge)	// Returns IntStream; change to method reference
				.filter(age -> age > 20)
				.average()	// Now you're able to call average
				.orElseThrow();	// No need to use OptionalDouble with this
		}

5.3 - Identifying the Three Steps of Refactoring Iterators to Streams
	Just a quick wrap up. This refactoring was a 3 step process:
		1. Spot the iteration and make it a stream
		2. Spot the mapping, and we had only one mapping for the person to the ages of those people
		3. Spot the filtering, and filtering was we only keep the age is greater than 20
	And at last, we had to check what reduction operation we're doing. Here, it was just a simple average computation, and for the average, you need to know that this is defined on a stream of numbers, a specialized stream, and that an average computation returns an Optional, wrapping the result

5.4 - Duplicating For Loops to Prepare for a Stream Refactoring
	Now we're going to see a more complex use case
	Example of use a use case where a for loop is doing multiple things - context is a movie rental
		public class MovieRental {
			private List<Rental> rentals = new ArrayList<>();
		
			main() {
				MovieRental movieRental = new MovieRental()	// List under the hood
				movieRental.addRental(...)
				...
				
				String statement = movieRental.statement();
				sysout(statement);
				
			}
			
			addRental(String name, int daysRented) {
				this.rentals.add(new Rental(name, daysRented));
			}
			
			public String statement() {
				double totalAmount = 0;
				int frequentRenterPoints = 0;
				String statement = composeHeader();	// Adds some string
				for(Rental rental: rentals) {
					totalAmount += computeRentalAmount(rental);
					frequentRenterPoints += getFrequentRenterPoints(rental);
					statement += composeStatementLine(rental);
				}
			}
		}
	Now you can see that I have this loop here with 3 computations conducted in one pass over the data, the data being this rentals list. The question is now, how can we refactor that? You can see that here, spotting the for loop is quite easy because I have only one, but spotting the mapping is a little tricky.
	Why? Becase you have multiple mappings
	So basically, this loop is doing 3 things and the Stream API is not very good at handling that. The Stream API always does compute one thing. So you should not try to convert that to a stream without first reorganizing this for loop:
		public String statement() {
			double totalAmount = 0;
			int frequentRenterPoints = 0;
			String statement = composeHeader();	// Adds some string
			for(Rental rental: rentals) {
				totalAmount += computeRentalAmount(rental);
			}
			for(Rental rental: rentals) {
				frequentRenterPoints += getFrequentRenterPoints(rental);
			}
			
			for(Rental rental: rentals) {
				statement += composeStatementLine(rental);
			}
		}
	Since the Stream API can only do one thing at a time, what we really need is not one for loop that does 3 things but 3 for loops that does one thing at a time
	You're probably thinking - I have one loop, and I'm conducting my processing on one pass over the data, and now I have 3 for loops, and I'm processing my data in 3 passes over the data. So I must have a major hit on performances
	So let me just tell you a principle, never sacrifice the readability of your code to the performance. The performance here is measured in nanoseconds, not more. If your application is doing some kind of database request or service request through microservices or web services, whatever, this is going to be measured in hundreds of microseconds, if not milliseconds
	So the performance you have here is really nothing compared to that. So this is fine. We're going to continue to carry one

5.5 - Refactoring Complex for Loops to Clean Stream Patterns
	public String statement() {
		double totalAmount = rentals.stream()
							.mapToDouble(this::computeRentalAmount)
							.sum();
		
		int frequentRenterPoints = rentals.stream()
										.mapToInt(this::getFrequentRenterPoints)
										.sum();
		
		String statement = composeHeader();
		statement += rentals.stream()
					.map(this::composeStatementLine)
					.collect(Collectors.joining());
	}
	For string, the aggregation is not an average or sum; it's the concatenation of all those strings of characters written by this composeStatementLine(). Now, to concatenate strings of characters, there is a trick; it uses a special reduction, which is called, in fact, a collection. So I need to call the collect() method and pass to this collect method a special object, which is a Collector. And to create a collector, I have a factory class called Collectors, and the Collectors.joining() is the collector that is going to concatenate the strings of characters for me

5.6 - Refactoring Iterators to Streams Wrap Up
	What we just saw in the second refactoring is a key point of the Stream API. The principle is a stream does one thing at a time, map, filter, one reduction.
	So when you ahve a for loop that is complex and that does too many things, your first goal should be to try to move this for loop to more than one for loop, several for loops, each for loop doing one thing at a time. And this is the condition you need to fulfill to be able to convert that to the Stream API
	But the result is really amazing, because at the end of the day, you do not have any for loop anymore, you just have a very clean stream with very obvious mapping, very readable filtering, and a reduction that most of the time will be using either the specialized aggregation of numbers or a collection
	And in order to achieve that, you need to forget about one idea - which is it is better to process your data in one pass over your data. In fact, this is such a wrong idea that even the JVM doesn't implement it for you. Most of the time when you have a for loop or when you have a stream, the JVM will optimize that for you and get rid of your for loop, get rid of your iteration, and you will have a zero pass of your data, just inline code, extremely performant, and extremely efficient
	So forget about this wrong idea and you'll be able to convert your for loop to the Stream API without losing any performance at all

6.1 - Introducing the Module and Its Agenda (Reducing Data to Compute Statistics)

6.2 - Implementing Reductions with a BinaryOperator
	What does it mean to reduce data? In the introduction of this course, I told you that reduction was, in a nutshell, some kind of aggregation in the SQL langauge sense, and we're going to keep that in mind
	We're going to take a very simple reduction, which is just the sum. And in fact, the sum is just modeled by a binary operator that takes 2 elements, suppose it's 2 integers and that returns the sum of those 2 elements
		BinaryOperator<Integer> sum = (i1, i2,) -> i1 + i2;
	How is this binary operator going to be used?
	Suppose we have a stream of integers, 3, 1, 4, 1,6. First, we're going to take the first 2 elements of that stream, apply this binary operator on these 2 elements, compute the sum, which in this case, is 4 is going to be the first element of the remaining integers to reduce
	And then we're going to apply that again and again until we have consumed all the elements of the stream
	So at the end of the day, that stream, we have computed a sum that is 15. If we take a closer look at what we've done, we realize that we've done this computation ((((3 + 1) + 4)+ 1)+ 6)
	This is how the computation of the sum has been made. It has been make by taking the elements 2 by 2
	This already may lead to some caveat. Suppose we want to compute the average. You could think that a binary operator to model the average could be this one:
		BinaryOperator<Integer> avg = (i1, i2) -> (i1 + i2) / 2;
	But if we apply this binary operator in the same way as we applied the sum operator on the same stream of integers, we're not going to get the average. Because what we're doing is ((((3 + 1)/2 + 4)/2 + 1)/2 + 6)/2
	So you need to be very cautious when you choose your binary operator because for the sum or the mean or the max, all those simple aggregations, choosing the right binary operator is very straightforward
	But for some, a little more complex operation, designing the right binary operator is really tricky and may lead to false results
	In fact, this binary operator needs to have a property called associativity and if it doesn't have this property, then you will have results that are probably wrong or probably unexpected, but no error whatsoever, neither in the compilation of your code, neither in the execution of your code
	So writing reduction operators in the form of a binary operator may lead to errors if the operators you write are not associative

6.3 - Introducing the Problem of the Reduction of Empty Streams
	Unfortunately, this is not the only caveat we may encounter when dealing with this reduction step
	What is going to happen when we need to reduce an empty string? We're going to see a first example, which is not completely empty, but which is a singleton. The algorithm I just described only works if I have at least 2 elements in the stream. If you have only one element, then you cannot apply this binary operator
	The real question is what is the sum of the elements of the stream if that stream is empty? And this is a real question

6.4 - Computing the Reduction of Empty Streams
	Let us consider a stream with a lot of data. And suppose that this data, we want to split it and conduct this computation in 2 phases. We have several calls on our computers, so maybe if this is the sum, we want to compute a partial sum of the first chunk of data and compute another partial sum of another chunk of data on another core of our CPU, maybe in parallel. And then we're going to merge the 2 partial results, applying the fact that we know that the sum of the sums is the sum of the overall
	We're going to do a little group theory here to really understand what's going on
	Suppose we have a set of integers A and you decide to divide this set of integers in 2 chunks, A1 and A2. What you're saying is that A is the union of A1 and A2. And then we'er going to say that the sum of all the elements of A is equal to the sum of the sums
		Sum(A1) = s1; Sum(A2) = s2;
	This is a very classical property of the sum, which is called the associativity. So what we're saying:
		Sum(A) = Sum(A1 U A2) = Sum(s1, s2)
	So let us just write this exact same line, but instead of using the sum, using a general reduction. What we're saying is:
		Red(A) = Red(A1 U A2) = Red(Red(A1), Red(A2))
	Remember, the sum is equal to the sum of the sums. What I'm just saying here is that the reduction is the reduction of the reductions
	But now, suppose that checking our set somehow went wrong. At the end of the day, it turns out that A2 is empty. It's not too bad because we know that our previous formula still applies right? A is still equal to A1 U A2, and if A2 is the empty set, then it means that A is equal to A1 U {}.
	So what we just wrote is simply A equals A union the empty set, which is always true by the way
	So now, let us apply the computation of this wrong division of A. What we see is
		Red(A) = Red(A U {}) = Red(Red(A), Red({}))
	Now we know that the reduction of the empty set reduced with the reduction of A should not change the value of the reduction of A. If it's the sum, it means that the sum of all the elements of A should be equal to the sum of all the elements of A, plus the sum of all the elements of the empty set. And this is true only if the sum of the elements of the empty set is equal to 0. Why? Because we know that 0 is the identity element of the sum operation
	So if I go back one step. Now, the answer is not "I don't know" anymore, the answer is really the identity element of the reduction operation. And this is the only justification of saying that the sum of an empty set is equal to 0 because 0 is the identity element of the sum reduction operation
	So this is really what you need to keep in mind, the reduction of am empty stream should be the identity element of the reduction operation

6.5 - Introducing Optional for Reductions with No Identity Element
	This is great because now I have solved a problem, but maybe more problems are coming because if you consider the max operator for instance, what is the identity element of the max operator? What integers will be such that if you take the max of any number and that identity element, it will return you that number? It turns out that the max operator doesn't have any identity element, and it is the same for the main operator, and it is also the same for the average operator
	We face reduction operations that do not any identity element. So how will the API handle that?
	What will be the return type of the max method in Java, knowing that if you take the max of any empty stream, then you cannot produce any result? This is precisely the reason why we need an Optional object along with the Stream API

6.6 - Implementing Reductions That Have an Identity Element
	Let us take a closer look at how this reduction operation has been implemented in the Stream API
	Now we know that the reduction of an empty stream should really return the identity element of the reduction operation, and we also know that some reduction operations do not have any identity element. It's the case for the min, max, and the average
	If we check the API precisely, we can see that that we have 2 reduce methods on the Stream<T> interface
	The first reduce method takes a first parameter, which has the type of the elements of the stream and then it takes this binary operator that we just saw. This first element should be the identity element of the reduction operation, reduction operation modeled by the binary operator.
	And if you pass an object that is not the identity element, then the result computed by this reduce call will be wrong
	And if you check the implementation of this reduce method, you will see that it takes into account the fact that this first element is the identity element. Example from before of BinaryOperator<Integer> sum - In fact, this reduce method operation doesn't take the first 2 elements to the stream to start the computation but merely it adds this identity element before the elements of the stream. So that if you have an empty stream, it will return this identity element, and if you only have one element in the stream, it will return the reduction of the identity element and this only element
	So this is really a key point. If you do not respect the contract of the reduce method, you will get wrong results

6.7 - Using Optional to Handle Reductions with No Identity Element
	There is a second version of this reduce method that doesn't take this identity element as the first parameter. The second version only takes the binary operator, which models the reduction operation
	And this reduce method, since it doesn't have any identity element, works differently
	In the case of the reduction of an empty stream, it cannot return this identity element anymore just because there is none. So the choice has been made to return an Optional element. So for Stream<T>, it returns an Optional<T>, and for the specialized streams of numbers, it returns the specialized corresponding Optional object
	So the first thing you need to understand is that Optional has been introduced in the Stream API to handle properly the reduction operations that do not have any identity element. An Optional object is just a wrapper. Inside this wrapper, I may have the proper result of the computation in case the stream was not empty. But if the stream is empty, then this wrapper is itself empty
	And it's a way for the API to tell the application code, I do not know what to give you as a result. So it becomes the responsibility of the application code to make some kind of decisions on what to return as a result, knowing that the reduction was made on an empty stream

6.8 - Getting the Content of an Optional
	So what can be done with this Optional object? There are many very interesting  methods to map filter and flap map optionals but they're beyond the scope of this course
	What you can do at least, is check for the presence or absence of an object inside this wrapper object. From Java 8 to 9, you can call the get() method to get the content of the optional
	And starting java 10, the preferred pattern is to call this orElseThrow() method to get the content of this optional
	In both cases, if you're calling a get() or orElseThrow() on an empty optional object, what you will get will be a NoSuchElementException
	The rule is if you're reducing a stream with an operation that do not have any identity element, then the method you're going to call will return an optional
	Here are the main methods that do return an optional because they do not have any identity element
	First, there is the reduce implementation that only takes the BinaryOperator without the identity element. And then, there is the min() and the max()
	Now there is a version of the min and max that takes a Comparator defined on Stream<T>, and another version that doesn't take any parameter defined on the specialized streams of numbers that we saw
	And then there is the average method that doesn't have any identity element neither defined on the stream of numbers

6.9 - Live Demo: Opening an Optional

6.10 - Live Demo: Passing a Wrong Identity Element to Reductions
	You really need to be extra careful when you're doing that and you really need to make sure that the identity element that you pass as the first parameter of this reduce method is indeed the identity element of your operation, because if it's not the case, you'll get buggy results with no error

6.11 - Live Demo: Computing a Max and a Min on a CSV File Using Streams

6.12 - Live Demo: Using a Specialized Stream to Compute Statistics
	Strema has a summaryStatistics() method that returns many common stats in one pass over the data

6.13 - Module Wrap Up
	You really should not consider that as a corner case (when there is no identity element). Very often you will see reduction operations that do not have any identity element
	This optional object is returned when there is no value that can be returned as a result of this reduction step. And this is one of the key points of the Stream API and probably one of the more subtle, one of the hardest points to understand. You will read in many places that Optional have been introduced in java just to handle the null value in another way. It is true that with Optional, you can handle null values in a very clean way, starting with Java 8, but this is not the reason why the Optional class has been introduced
	The real reason why is to end all reduction operations that do not have any identity element

7.1 - Introducing the Module and Its Agenda (Collecting Data from Streams to Create Lists and Sets)
	A collector is quite a complex subject, which is used to reduce the data processed by a stream in a special way. In fact, a collector can gather data, can collect data, in a container that maybe a collection, so any kind of collection - a list, a set or even a map, and also strings of characters
	In the documentation, this is called reduction in a mutable container
	So far, the reductions we saw were merely aggregations in a SQL sense. With this collector API, we're going to go one step further and collect data in a quite sophisticated in-memory structures

7.2 - Using a Collector to Reduce a Stream in a Mutable Container
	Let us collect data in collections. So this path relies on a specail API, which is linked to the Stream API and which is called the Collector API
	The entry point from the stream API perspective is a special method on the stream interface called Stream.collect() method
	This collect() method takes an instance of a collector as a parameter, and this collector is an implementation of the Collector interface
	Creating your own collector is doable, but it's a little complex, it's a little tricky, and you need to be a little bit careful with that, because there are cabeats and corner cases that you need to handle and this is beyond the scope of this course
	 But we have a Collectors factory class with many factory methods on it, which are going to give you ready to use collectors without you having to implement this interface yourself

7.3 - Reducing Stream Data in a List, a Set, or in Any Collection
	How can you create a list with the element processed by your stream? How can you gather those elements in the list? With everything we saw so far in this API, you may be tempted to write this kind of code:
		List<Person> peopleFromNewYork = new ArrayList();
		
		people.stream()
			.filter(p -> p.getCity().equals("New York"))
			.forEach(p -> peopleFromNewYork.add(p));
	Now, if I'm mentioning this way of writing things, it's just to give you one advice, do NOT use it. This is not a pattern. This is a wrong pattern. This is an anti-pattern, so do not create your lists in that way
	The right way of putting the data processed by your stream is this pattern, call the collect() method:
		List<Person> peopleFromNewYork =
			people.stream()
				.filter(p -> p.getCity().equals("New York"))
				.collect(Collectors.toList());
	Now what you're doing really is calling a factory method called toList() of this factory class called Collectors and this factory method is going to create a collector for you that will gather all the elements of this stream in a list. And that's it, this pattern is extremely simply and this is the right pattern to use if you want to put the elements of your streams in a list
	ArrayList is not the only mutable container in which you can put your element, you can also put them in a set, and here is the pattern:
		Set<Person> peopleFromParis =
			people.stream()
				.filter(p -> p.getCity().equals("Paris"))
				.collect(Collectors.toSet());
	You'll just use Collectors.toSet()
	And if you have a homemade lists if your application, you can also use them by calling the Collectors.toCollection() and by providing a sublayer that is going to create the mutable container that you want to put your data in
		MyCollection<Person> peopleFromLondon =
			people.stream()
				.filter(p -> p.getCity().equals("London"))
				.collect(Collectors.toCollection(MyCollection::new));
	So, even if you have your own implementation of the List interface or if you want to use third party implementations that are not part of the JDK, you can do that by calling this special factory method
	And the last pattern that we're going to see is the joining pattern. You can put your elements in any kind of collections, a list, set or homemade collections, you can also concatenate the different elements of a stream in a single string of characters
		String names =
			people.stream()
				.filter(p-> p.getCity().equals("San Francisco"))
				.map(p -> p.getName())
				.collect(Collectors.joining(", "));
	Now this only works if you ahve a stream of strings of character. It doesn't work for a Stream of T of any object. But it's very easy to map a stream of T to a sream of strings and then use these collectors joining to do that
	Now these characters that are joining may take several parameters, and the first parameter is a separator to separate to different elements you want to concatenate

7.4 - Live Demo: Collecting a Stream in a List, a Set, or an Array
	Example of reading a file containing city and population data - I could try to put this in an array. This is a useful pattern we haven't seen so far
	To put a stream in an arrya, I may use this toArray() method:
		Object[] array = cities.stream().toArray();
	But if I use it that way, what I get is an array of object. This is due to the way the type system works in java and works with the generics. If I want to have an array of Strings because this is a Stream<String>, I need to pass a special function here a as a parameter to this toArray method
	And in fact, there is a trick. There is a very simple way to write this function
		String[] array = cities.stream()toArray(String::new);
	All you have to do is take the type of the elements, make it an array, and just type in "::new". This is a method reference. Whatever the type is of this method reference, it's always going to work

7.5 - Live Demo: Joining a Stream of String with a Collector
	Multiple signature definitions of joining()
	joining() with no parameters - no separator between elements
	joining(first, second, third) - second and third refers to how to start and end the string

8.1 - Introducing the Module and its Agenda (Creating and Analyzing Histograms from Streams)
	In this module, we're going to go one step further with this Collector API
	The goal of this module is to show you how you can create and manipulate maps with this Collector API and the collector we're going to use is the groupingBy() collector. This collector is a little tricky to use because it can take a further collector as a parameter, which is called the downstream collector
	And everything interesting you can do as the groupingBy() collector relies on the proper use of this downstream collector

8.2 - Creating Maps with the GroupingBy Collector
	How does this groupingBy() collector work? How can you create maps using it? Let us see that on example with CSV file that has cities, state and population info
	We have a short list of cities and from within the US and along with those cities we have the states. What we want to do is regroup those cities by their state
		cities.stream()
			.collect(
				Collectors.groupingBy(
					city -> city.getState()
				)
			);
	groupingBy() works with a special function. This function is going to take an object from the stream, here one of the city, and will extract from that object, will compute from that object a special property, which is going to be used to create the key of that map. And in our example, this is going to be the state of that city
	We may assume that more than one element of the stream is going to generate the same key, and this is the case in this example
	The groupingBy collector knows that and the behavior is the following: all the elements of the stream that have been generated the same key will be put in a list in the same list associated with that key. So the map generated by this groupingBy collector is following:
		Map<String, List<City>>
	So the first behavior of the groupingBy collector is it takes the object of the stream, passes it through a key extractor, which is a function, and creates lists associated with the key computed by this key extractor

8.3 - Adding a Downstream Collector to the GroupingBy Collector
	Now, what makes this groupingBy() collector so amazing is the possibility you have to give another collector to the groupingBy, and this other collector is called the downstream collector
	What is the groupingBy going to do with this downstream collector? It is going to apply this collector to the streaming of list of values it has computed
	So suppose that the downstream collector we pass to the produce patter nis the Collectors.counting(). This is one of the most simple collectors you may image. It just counts the number of elements processed by your given stream
	So if you pass this Collectors.counting() as a downstream collector to the previous groupingBy(), the groupingBy() is going to take it, will stream that list of values, and collect this stream using this Collectors.counting(), thus computing the number of elements processed by that stream.

8.4 - Live Demo: Grouping the Cities by Their State

8.5 - Live Demo: Creating the Histogram of Number of Cities per State

8.6 - Live Demo: Analyzing Histograms by Streaming Maps with EntrySet
	Collectors.summingInt()
	So you can see that this downstream collector really makes this groupingBy() collector extremely powerful. You can basically create any map you need as complex as you need, and once this map is made, you can extract the information you need by taking your max over the values or over the keys
	This is an extremely powerful pattern and you can really process your data very efficiently with this groupingBy()
